# -*- coding: utf-8 -*-
"""Revised Fun Sim.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uVq75vLNuxr_y8UIfoyKY3b3nT2WdinZ
"""

from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

import pandas as pd

file_path = '/content/drive/MyDrive/Fun Sim/Functional_Score11.xlsx'
F11 = pd.read_excel(file_path)

file_path = '/content/drive/MyDrive/Fun Sim/Functional_Score22.xlsx'
F22 = pd.read_excel(file_path)

file_path = '/content/drive/MyDrive/Fun Sim/Functional_Score33.xlsx'
F33 = pd.read_excel(file_path)

combined_df = pd.concat([F11, F22, F33], ignore_index=True)
print(combined_df)

# Check for duplicate rows
duplicate_rows = combined_df[combined_df.duplicated()]

# Display duplicate rows
print("Duplicate Rows except first occurrence:")
print(duplicate_rows)

# To count the number of duplicate rows
num_duplicates = combined_df.duplicated().sum()
print(f"\nNumber of duplicate rows: {num_duplicates}")

# Removing Duplicate Rows
combined_df_u = combined_df.drop_duplicates()
print(combined_df_u)

# Append node2 to node1
combined_nodes = pd.concat([combined_df_u['node1'], combined_df_u['node2']])

# Find unique nodes
unique_nodes = combined_nodes.unique()

print(len(unique_nodes))

max_value1 = combined_df_u['score'].max()
min_value1 = combined_df_u['score'].min()
print("Maximum value :", max_value1)
print("Minimum value :", min_value1)
Lmin = 1.48533843557336
Lmax = 5.98754487611962

def normalize_lls(row):
    LLS_min = 1.48533843557336
    LLS_max = 5.98754487611962
    return (row['score'] - LLS_min) / (LLS_max - LLS_min)

# Create a new column 'Normalized_LLS' using .loc
combined_df_u['Normalized_LLS'] = combined_df_u.apply(lambda row: normalize_lls(row), axis=1)

# Alternatively, you can use .loc directly
# combined_df_u['Normalized_LLS'] = combined_df_u.apply(lambda row: normalize_lls(row), axis=1)

# Display the DataFrame
print(combined_df_u)

import pandas as pd
# Identify unique nodes from both columns
unique_nodes = set(combined_df_u['node1']).union(set(combined_df_u['node2']))
print(len(unique_nodes))

# Create a new DataFrame to store the expanded data
expanded_data = {'node1': [], 'node2': [], 'score': [], 'Normalized_LLS': []}

# Iterate through unique nodes
for node in unique_nodes:
    # Append rows with the same node in both columns and a score of 1
    expanded_data['node1'].append(node)
    expanded_data['node2'].append(node)
    expanded_data['score'].append(1)
    expanded_data['Normalized_LLS'].append(1)

# Create a new DataFrame from the expanded data and concatenate it with the original DataFrame
df = pd.concat([combined_df_u, pd.DataFrame(expanded_data)], ignore_index=True)

# Display the new DataFrame
print(df)

import pandas as pd
# Load Excel file from Google Drive
file_path = '/content/drive/MyDrive/Fun Sim/DOID and GENE ID.xlsx'
SGID = pd.read_excel(file_path)
print(SGID)

import pandas as pd

# Function to filter rows based on Disease ID and create a new DataFrame
def filter_by_disease_id(df, disease_id):
    mask = df['DOID'] == disease_id
    result_df = df[mask].copy()
    return result_df

# Function to calculate Functional Similarity between two diseases
def calculate_functional_similarity(disease_id1, disease_id2, SGID, df):
    D1 = filter_by_disease_id(SGID, disease_id1)
    n = len(D1)
    print(n)

    D2 = filter_by_disease_id(SGID, disease_id2)
    m = len(D2)
    print(m)

    # Initialize max_score_sum
    max_score_sum = 0

    # Iterate through elements of D1 and calculate maximum score sum
    for _, row1 in D1.iterrows():
        g1_element = row1['Gene ID']
        max_score = 0

        for _, row2 in D2.iterrows():
            g2_element = row2['Gene ID']

            subset = df[((df['node1'] == g1_element) & (df['node2'] == g2_element))]
            if not subset.empty:
                score = subset['Normalized_LLS'].values[0]
                if score > max_score:
                    max_score = score

        max_score_sum += max_score

    # Initialize max_score_sum
    max_score_sum11 = 0

    # Iterate through elements of D2 and calculate maximum score sum
    for _, row1 in D2.iterrows():
        g1_element = row1['Gene ID']
        max_score = 0

        for _, row2 in D1.iterrows():
            g2_element = row2['Gene ID']

            subset = df[((df['node1'] == g1_element) & (df['node2'] == g2_element))]
            if not subset.empty:
                score = subset['Normalized_LLS'].values[0]
                if score > max_score:
                    max_score = score

        max_score_sum11 += max_score

    # Calculate Functional Similarity
    Fun_Sim = (max_score_sum + max_score_sum11) / (m + n)

    return Fun_Sim

# Example usage of the function
disease_id1 = 'DOID:7148'
disease_id2 = 'DOID:2349'
fun_sim_result = calculate_functional_similarity(disease_id1, disease_id2, SGID, df)
print(f"Functional Similarity between {disease_id1} and {disease_id2}: {fun_sim_result}")

import pandas as pd

# Function to filter rows based on Disease ID and create a new DataFrame
def filter_by_disease_id(df, disease_id):
    mask = df['DOID'] == disease_id
    result_df = df[mask].copy()
    return result_df

# Function to calculate Functional Similarity between two diseases
def calculate_functional_similarity(disease_id1, disease_id2, SGID, df):
    D1 = filter_by_disease_id(GID, disease_id1)
    n = len(D1)

    D2 = filter_by_disease_id(GID, disease_id2)
    m = len(D2)

    # Initialize max_score_sum
    max_score_sum = 0

    # Iterate through elements of D1 and calculate maximum score sum
    for _, row1 in D1.iterrows():
        g1_element = row1['Gene ID']
        max_score = 0

        for _, row2 in D2.iterrows():
            g2_element = row2['Gene ID']

            subset = df[((df['node1'] == g1_element) & (df['node2'] == g2_element))]
            if not subset.empty:
                score = subset['Normalized_LLS'].values[0]
                if score > max_score:
                    max_score = score

        max_score_sum += max_score

    # Initialize max_score_sum
    max_score_sum11 = 0

    # Iterate through elements of D2 and calculate maximum score sum
    for _, row1 in D2.iterrows():
        g1_element = row1['Gene ID']
        max_score = 0

        for _, row2 in D1.iterrows():
            g2_element = row2['Gene ID']

            subset = df[((df['node1'] == g1_element) & (df['node2'] == g2_element))]
            if not subset.empty:
                score = subset['Normalized_LLS'].values[0]
                if score > max_score:
                    max_score = score

        max_score_sum11 += max_score

    # Calculate Functional Similarity
    Fun_Sim = (max_score_sum + max_score_sum11) / (m + n)

    return Fun_Sim

# Function to create similarity matrix for unique diseases
def create_similarity_matrix(unique_diseases, GID, df, output_file):
    num_diseases = len(unique_diseases)
    similarity_matrix = pd.DataFrame(index=unique_diseases, columns=unique_diseases)

    for i in range(num_diseases):
        for j in range(i+1, num_diseases):
            disease_id1 = unique_diseases[i]
            disease_id2 = unique_diseases[j]

            fun_sim_result = calculate_functional_similarity(disease_id1, disease_id2, GID, df)
            similarity_matrix.at[disease_id1, disease_id2] = fun_sim_result
            similarity_matrix.at[disease_id2, disease_id1] = fun_sim_result

    # Save the similarity matrix to a specified location
    similarity_matrix.to_csv(output_file)

# Example usage
# Replace 'unique_diseases', 'SGID', 'df', and 'output_file' with your actual data and desired output file path
unique_diseases = ['DOID:10652', 'DOID:2355','DOID:2841','DOID:3312','DOID:12930','DOID:3083','DOID:12558','DOID:9351','DOID:289','DOID:1826','DOID:12858','DOID:13809','DOID:12336','DOID:9455','DOID:12365','DOID:1596','DOID:2377','DOID:5844','DOID:898','DOID:11612','DOID:7148','DOID:11335','DOID:12176','DOID:2349','DOID:8469','DOID:9588','DOID:1936','DOID:848','DOID:6132','DOID:633','DOID:2352','DOID:1564','DOID:552','DOID:615','DOID:83','DOID:11476','DOID:5419','DOID:9970','DOID:1485','DOID:1459','DOID:326','DOID:9471','DOID:9120','DOID:423','DOID:419','DOID:4989','DOID:0050700']  # Replace with your actual unique disease IDs
#SGID = pd.DataFrame(...)  # Replace with your actual SGID dataframe
#df = pd.DataFrame(...)  # Replace with your actual df dataframe
output_file = '/content/similarity_matrix.csv'

create_similarity_matrix(unique_diseases, GID, df, output_file)

import pandas as pd
import numpy as np

# Load the first Excel file into a DataFrame and set 'DOID' as the index
file_path1 = '/content/drive/MyDrive/Fun Sim/Functional_Similarity.xlsx'
Functional_Similarity = pd.read_excel(file_path1).set_index('DOID')
print(Functional_Similarity)

file_path2 = '/content/drive/MyDrive/Integration/true positives.xlsx'
actual_df = pd.read_excel(file_path2).set_index('DOID')
print(actual_df)

import pandas as pd
import io

# Assuming similarity_matrix_semfunsim is already loaded

# Min-Max normalization function
def min_max_normalize(matrix):
    matrix_array = matrix.values.astype(float)
    min_val = matrix_array.min()
    print(min_val)
    max_val = matrix_array.max()
    print(max_val)
    normalized_matrix = (matrix_array - min_val) / (max_val - min_val)
    return normalized_matrix

# Normalize the similarity matrix
normalized_similarity_matrix = min_max_normalize(Functional_Similarity)

# Create a DataFrame for the normalized matrix
predicted_df = pd.DataFrame(normalized_similarity_matrix, columns=Functional_Similarity.columns, index=Functional_Similarity.index)

# Export the normalized matrix to an Excel file
#normalized_df.to_excel("normalized_matrix_jaccard.xlsx")

# Display the normalized matrix DataFrame
#print("Normalized Similarity Matrix:")
#print(predicted_df)

import pandas as pd
from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

# Assuming you have actual_df and predicted_df dataframes

# Combine the actual and predicted values for each pair of diseases
actual_values = actual_df.values.flatten()
predicted_values = predicted_df.values.flatten()

# Compute the ROC curve
fpr, tpr, thresholds = roc_curve(actual_values, predicted_values)


# Compute the Area Under the Curve (AUC)
roc_auc = auc(fpr, tpr)

# Plot the ROC curve
plt.figure(figsize=(10, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Functional Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc='lower right')
plt.show()

import pandas as pd
import numpy as np
from sklearn.metrics import roc_curve, auc

# Assuming you have actual_df and predicted_df dataframes
actual_values = actual_df.values.flatten()
predicted_values = predicted_df.values.flatten()

# Compute the ROC curve
fpr, tpr, thresholds = roc_curve(actual_values, predicted_values)

# Calculate Youden's J statistic for each threshold
youden_j = tpr - fpr
best_threshold_index = np.argmax(youden_j)
best_threshold = thresholds[best_threshold_index]

# Print the best threshold
print(f"Best Threshold: {best_threshold}")

# Compute the Area Under the Curve (AUC)
roc_auc = auc(fpr, tpr)

# Plot the ROC curve
plt.figure(figsize=(10, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.scatter(fpr[best_threshold_index], tpr[best_threshold_index], marker='o', color='red', label=f'Best Threshold: {best_threshold:.2f}')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Functional Similarity ROC Curve')
plt.legend(loc='lower right')
plt.show()

"""**Integration (Functional * Semantic)**"""

file_path3 = '/content/drive/MyDrive/Integration/semfunsim_matrix_final - Copy.xlsx'
semfunsim_df = pd.read_excel(file_path3).set_index('DOID')
print(semfunsim_df)

file_path1 = '/content/drive/MyDrive/Fun Sim/Functional_Similarity.xlsx'
Functional_df= pd.read_excel(file_path1).set_index('DOID')
print(Functional_df)

# Perform element-wise multiplication excluding the index column
result = semfunsim_df.values * Functional_df.values

# Create a new DataFrame with the same index and columns
result_df_1 = pd.DataFrame(result, index=semfunsim_df.index, columns=semfunsim_df.columns)

# Print the result
print(result_df_1)

# Normalize the similarity matrix
normalized_similarity_matrix1 = min_max_normalize(result_df_1)

# Create a DataFrame for the normalized matrix
predicted_n_df = pd.DataFrame(normalized_similarity_matrix1, columns=semfunsim_df.columns, index=semfunsim_df.index)
print(predicted_n_df)

import pandas as pd
import numpy as np
from sklearn.metrics import roc_curve, auc

# Assuming you have actual_df and predicted_df dataframes
actual_values = actual_df.values.flatten()
predicted_values = predicted_n_df.values.flatten()

# Compute the ROC curve
fpr, tpr, thresholds = roc_curve(actual_values, predicted_values)

# Calculate Youden's J statistic for each threshold
youden_j = tpr - fpr
best_threshold_index = np.argmax(youden_j)
best_threshold = thresholds[best_threshold_index]

# Print the best threshold
print(f"Best Threshold: {best_threshold}")

# Compute the Area Under the Curve (AUC)
roc_auc = auc(fpr, tpr)

# Plot the ROC curve
plt.figure(figsize=(10, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.scatter(fpr[best_threshold_index], tpr[best_threshold_index], marker='o', color='red', label=f'Best Threshold: {best_threshold:.2f}')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Integration (Functional * Semantic) ROC Curve')
plt.legend(loc='lower right')
plt.show()

"""**Integration (sqrt(Functional * Semantic))**"""

import numpy as np

file_path3 = '/content/drive/MyDrive/Integration/semfunsim_matrix_final - Copy.xlsx'
semfunsim_df = pd.read_excel(file_path3).set_index('DOID')
print(semfunsim_df)

file_path1 = '/content/drive/MyDrive/Fun Sim/Functional_Similarity.xlsx'
Functional_df= pd.read_excel(file_path1).set_index('DOID')
print(Functional_df)

# Perform element-wise multiplication excluding the index column
result = np.sqrt(semfunsim_df.values * Functional_df.values)

# Create a new DataFrame with the same index and columns
result_df_11 = pd.DataFrame(result, index=semfunsim_df.index, columns=semfunsim_df.columns)

# Print the result
print(result_df_11)

# Normalize the similarity matrix
normalized_similarity_matrix11 = min_max_normalize(result_df_11)

# Create a DataFrame for the normalized matrix
predicted_n_df_1 = pd.DataFrame(normalized_similarity_matrix11, columns=semfunsim_df.columns, index=semfunsim_df.index)
print(predicted_n_df_1)

import pandas as pd
import numpy as np
from sklearn.metrics import roc_curve, auc

# Assuming you have actual_df and predicted_df dataframes
actual_values = actual_df.values.flatten()
predicted_values = predicted_n_df_1.values.flatten()

# Compute the ROC curve
fpr, tpr, thresholds = roc_curve(actual_values, predicted_values)

# Calculate Youden's J statistic for each threshold
youden_j = tpr - fpr
best_threshold_index = np.argmax(youden_j)
best_threshold = thresholds[best_threshold_index]

# Print the best threshold
print(f"Best Threshold: {best_threshold}")

# Compute the Area Under the Curve (AUC)
roc_auc = auc(fpr, tpr)

# Plot the ROC curve
plt.figure(figsize=(10, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.scatter(fpr[best_threshold_index], tpr[best_threshold_index], marker='o', color='red', label=f'Best Threshold: {best_threshold:.2f}')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Integration SQRT(Functional * Semantic) ROC Curve')
plt.legend(loc='lower right')
plt.show()

"""**Integration (Functional*Semantic*Network)**"""

import numpy as np

file_path3 = '/content/drive/MyDrive/Integration/semfunsim_matrix_final - Copy.xlsx'
semfunsim_df = pd.read_excel(file_path3).set_index('DOID')
print(semfunsim_df)

file_path1 = '/content/drive/MyDrive/Fun Sim/Functional_Similarity.xlsx'
Functional_df= pd.read_excel(file_path1).set_index('DOID')
print(Functional_df)

# Load the second Excel file into a DataFrame and set 'DOID' as the index
file_path5 = '/content/drive/MyDrive/Integration/CSM.xlsx'
CSM_df = pd.read_excel(file_path5).set_index('DOID')
print(CSM_df)

# Perform element-wise multiplication excluding the index column
result = (semfunsim_df.values * Functional_df.values * CSM_df.values)

# Create a new DataFrame with the same index and columns
result_df_111 = pd.DataFrame(result, index=semfunsim_df.index, columns=semfunsim_df.columns)

# Print the result
print(result_df_111)

# Normalize the similarity matrix
normalized_similarity_matrix111 = min_max_normalize(result_df_111)

# Create a DataFrame for the normalized matrix
predicted_n_df_11 = pd.DataFrame(normalized_similarity_matrix111, columns=semfunsim_df.columns, index=semfunsim_df.index)
print(predicted_n_df_11)

import pandas as pd
import numpy as np
from sklearn.metrics import roc_curve, auc

# Assuming you have actual_df and predicted_df dataframes
actual_values = actual_df.values.flatten()
predicted_values = predicted_n_df_11.values.flatten()

# Compute the ROC curve
fpr, tpr, thresholds = roc_curve(actual_values, predicted_values)

# Calculate Youden's J statistic for each threshold
youden_j = tpr - fpr
best_threshold_index = np.argmax(youden_j)
best_threshold = thresholds[best_threshold_index]

# Print the best threshold
print(f"Best Threshold: {best_threshold}")

# Compute the Area Under the Curve (AUC)
roc_auc = auc(fpr, tpr)

# Plot the ROC curve
plt.figure(figsize=(10, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.scatter(fpr[best_threshold_index], tpr[best_threshold_index], marker='o', color='red', label=f'Best Threshold: {best_threshold:.2f}')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Integration (Functional * Semantic * Network) ROC Curve')
plt.legend(loc='lower right')
plt.show()

"""**Integration SQRT(Functional * Semantic * Network)**

"""

import numpy as np

file_path3 = '/content/drive/MyDrive/Integration/semfunsim_matrix_final - Copy.xlsx'
semfunsim_df = pd.read_excel(file_path3).set_index('DOID')
print(semfunsim_df)

file_path1 = '/content/drive/MyDrive/Fun Sim/Functional_Similarity.xlsx'
Functional_df= pd.read_excel(file_path1).set_index('DOID')
print(Functional_df)

# Load the second Excel file into a DataFrame and set 'DOID' as the index
file_path5 = '/content/drive/MyDrive/Integration/CSM.xlsx'
CSM_df = pd.read_excel(file_path5).set_index('DOID')
print(CSM_df)

# Perform element-wise multiplication excluding the index column
result = np.sqrt(semfunsim_df.values * Functional_df.values + CSM_df.values)

# Create a new DataFrame with the same index and columns
result_df_1111 = pd.DataFrame(result, index=semfunsim_df.index, columns=semfunsim_df.columns)

# Print the result
print(result_df_1111)

# Export result_df_1111 to Excel
output_file_path = '/content/result_df_1111.xlsx'
result_df_1111.to_excel(output_file_path)

print(f"DataFrame exported to {output_file_path}")

# Normalize the similarity matrix
normalized_similarity_matrix1111 = min_max_normalize(result_df_1111)

# Create a DataFrame for the normalized matrix
predicted_n_df_111 = pd.DataFrame(normalized_similarity_matrix1111, columns=semfunsim_df.columns, index=semfunsim_df.index)
print(predicted_n_df_111)

# Export result_df_1111 to Excel
output_file_path = '/content/result_df_1111.xlsx'
predicted_n_df_111.to_excel(output_file_path)

print(f"DataFrame exported to {output_file_path}")

import pandas as pd
import numpy as np
from sklearn.metrics import roc_curve, auc

# Assuming you have actual_df and predicted_df dataframes
actual_values = actual_df.values.flatten()
predicted_values = predicted_n_df_111.values.flatten()

# Compute the ROC curve
fpr, tpr, thresholds = roc_curve(actual_values, predicted_values)

# Calculate Youden's J statistic for each threshold
youden_j = tpr - fpr
best_threshold_index = np.argmax(youden_j)
best_threshold = thresholds[best_threshold_index]

# Print the best threshold
#print(f"Best Threshold: {best_threshold}")

# Compute the Area Under the Curve (AUC)
roc_auc = auc(fpr, tpr)

# Plot the ROC curve
plt.figure(figsize=(10, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'Cheng (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.scatter(fpr[best_threshold_index], tpr[best_threshold_index], marker='o', color='red')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve ((Functional * Semantic ) + Network) ')
plt.legend(loc='lower right')
plt.show()

"""**
Integration (a* Functional + (1-a)*semantic)**
"""

import numpy as np
import pandas as pd

file_path3 = '/content/drive/MyDrive/Integration/semfunsim_matrix_final - Copy.xlsx'
semfunsim_df = pd.read_excel(file_path3).set_index('DOID')
print(semfunsim_df)

file_path1 = '/content/drive/MyDrive/Fun Sim/Functional_Similarity.xlsx'
Functional_df= pd.read_excel(file_path1).set_index('DOID')
print(Functional_df)

file_path8 = '/content/drive/MyDrive/Integration/true positives.xlsx'
actual_df= pd.read_excel(file_path8).set_index('DOID')
print(actual_df)

file_path8 = '/content/drive/MyDrive/Integration/CSM.xlsx'
CSM_df= pd.read_excel(file_path8).set_index('DOID')
print(CSM_df)

# Perform element-wise multiplication excluding the index column
result = (0.2 * semfunsim_df.values)  * (0.6 * Functional_df.values) + (0.2 * CSM_df.values)

# Create a new DataFrame with the same index and columns
result_df_22 = pd.DataFrame(result, index=semfunsim_df.index, columns=semfunsim_df.columns)

# Print the result
print(result_df_22)

# Normalize the similarity matrix
normalized_similarity_matrix22 = min_max_normalize(result_df_22)


# Create a DataFrame for the normalized matrix
predicted_n_df_22 = pd.DataFrame(normalized_similarity_matrix22, columns=semfunsim_df.columns, index=semfunsim_df.index)
print(predicted_n_df_22)

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc

# Assuming you have actual_df and predicted_df dataframes
actual_values = actual_df.values.flatten()
predicted_values = predicted_n_df_22.values.flatten()

# Compute the ROC curve
fpr, tpr, thresholds = roc_curve(actual_values, predicted_values)

# Calculate Youden's J statistic for each threshold
youden_j = tpr - fpr
best_threshold_index = np.argmax(youden_j)
best_threshold = thresholds[best_threshold_index]

# Print the best threshold
print(f"Best Threshold: {best_threshold}")

# Compute the Area Under the Curve (AUC)
roc_auc = auc(fpr, tpr)

# Plot the ROC curve
plt.figure(figsize=(10, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.scatter(fpr[best_threshold_index], tpr[best_threshold_index], marker='o', color='red', label=f'Best Threshold: {best_threshold:.2f}')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('(0.2 * Semantic) * (0.6 * Functional) + (0.2 * Network) ROC Curve')
plt.legend(loc='lower right')
plt.show()

threshold_value = 0.005554538653366527  # Set your desired threshold here

# Extract the numeric part of the matrix
numeric_matrix = predicted_n_df_22.iloc[:, 1:].values.astype(float)

# Apply thresholding
thresholded_matrix = np.where(numeric_matrix >= threshold_value, 1, 0)

# Create a new DataFrame from the thresholded matrix with the same index and columns
thresholded_df = pd.DataFrame(thresholded_matrix, index=predicted_n_df_22.index, columns=predicted_n_df_22.columns[1:])

# Display the thresholded DataFrame
print(thresholded_df)

thresholded_df.to_excel('thresholded_integration.xlsx')

import numpy as np
import pandas as pd

file_path1 = '/content/drive/MyDrive/Fun Sim/Functional_Similarity.xlsx'
Fun_df= pd.read_excel(file_path1).set_index('DOID')
print(Functional_df)

file_path8 = '/content/drive/MyDrive/Integration/true positives.xlsx'
actual_df= pd.read_excel(file_path8).set_index('DOID')
print(actual_df)

# Normalize the similarity matrix
normalized_similarity_matrix = min_max_normalize(Fun_df)

# Create a DataFrame for the normalized matrix
predict = pd.DataFrame(normalized_similarity_matrix, columns=Fun_df.columns, index=Fun_df.index)
print(predict)


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc

# Assuming you have actual_df and predicted_df dataframes
actual_values = actual_df.values.flatten()
predicted_values = predict.values.flatten()

# Compute the ROC curve
fpr, tpr, thresholds = roc_curve(actual_values, predicted_values)

# Calculate Youden's J statistic for each threshold
youden_j = tpr - fpr
best_threshold_index = np.argmax(youden_j)
best_threshold = thresholds[best_threshold_index]

# Print the best threshold
print(f"Best Threshold: {best_threshold}")

# Compute the Area Under the Curve (AUC)
roc_auc = auc(fpr, tpr)

# Plot the ROC curve
plt.figure(figsize=(10, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.scatter(fpr[best_threshold_index], tpr[best_threshold_index], marker='o', color='red', label=f'Best Threshold: {best_threshold:.2f}')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('FUN SIM ROC CURVE')
plt.legend(loc='lower right')
plt.show()

threshold_value = 0.256819184702824  # Set your desired threshold here

# Extract the numeric part of the matrix
numeric_matrix1 = predict.iloc[:, 1:].values.astype(float)

# Apply thresholding
thresholded_matrix = np.where(numeric_matrix1 >= threshold_value, 1, 0)

# Create a new DataFrame from the thresholded matrix with the same index and columns
thresholded_df = pd.DataFrame(thresholded_matrix, index=predict.index, columns=predict.columns[1:])

# Display the thresholded DataFrame
print(thresholded_df)

thresholded_df.to_excel('thresholded_FUNSIM.xlsx')

import numpy as np
import pandas as pd

file_path1 = '/content/drive/MyDrive/Integration/CSM.xlsx'
CSM_df= pd.read_excel(file_path1).set_index('DOID')
print(Functional_df)

file_path8 = '/content/drive/MyDrive/Integration/true positives.xlsx'
actual_df= pd.read_excel(file_path8).set_index('DOID')
print(actual_df)

# Normalize the similarity matrix
normalized_similarity_matrix1 = min_max_normalize(CSM_df)

# Create a DataFrame for the normalized matrix
predict1 = pd.DataFrame(normalized_similarity_matrix1, columns=CSM_df.columns, index=CSM_df.index)
print(predict1)


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc

# Assuming you have actual_df and predicted_df dataframes
actual_values = actual_df.values.flatten()
predicted_values = predict1.values.flatten()

# Compute the ROC curve
fpr, tpr, thresholds = roc_curve(actual_values, predicted_values)

# Calculate Youden's J statistic for each threshold
youden_j = tpr - fpr
best_threshold_index = np.argmax(youden_j)
best_threshold = thresholds[best_threshold_index]

# Print the best threshold
print(f"Best Threshold: {best_threshold}")

# Compute the Area Under the Curve (AUC)
roc_auc = auc(fpr, tpr)

# Plot the ROC curve
plt.figure(figsize=(10, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.scatter(fpr[best_threshold_index], tpr[best_threshold_index], marker='o', color='red', label=f'Best Threshold: {best_threshold:.2f}')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('CSM ROC CURVE')
plt.legend(loc='lower right')
plt.show()

threshold_value = 0.9924899704088883  # Set your desired threshold here

# Extract the numeric part of the matrix
numeric_matrix11 = predict1.iloc[:, 1:].values.astype(float)

# Apply thresholding
thresholded_matrix1 = np.where(numeric_matrix11 >= threshold_value, 1, 0)

# Create a new DataFrame from the thresholded matrix with the same index and columns
thresholded_df1 = pd.DataFrame(thresholded_matrix1, index=predict1.index, columns=predict1.columns[1:])

# Display the thresholded DataFrame
print(thresholded_df1)

thresholded_df1.to_excel('thresholded_CSM.xlsx')

import numpy as np
import pandas as pd


file_path3 = '/content/drive/MyDrive/Integration/semfunsim_matrix_final - Copy.xlsx'
Sem_df = pd.read_excel(file_path3).set_index('DOID')
print(Sem_df)

file_path8 = '/content/drive/MyDrive/Integration/true positives.xlsx'
actual_df= pd.read_excel(file_path8).set_index('DOID')
print(actual_df)

# Normalize the similarity matrix
normalized_similarity_matrix11 = min_max_normalize(Sem_df)

# Create a DataFrame for the normalized matrix
predict11 = pd.DataFrame(normalized_similarity_matrix11, columns=Sem_df.columns, index=Sem_df.index)
print(predict11)


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc

# Assuming you have actual_df and predicted_df dataframes
actual_values = actual_df.values.flatten()
predicted_values = predict11.values.flatten()

# Compute the ROC curve
fpr, tpr, thresholds = roc_curve(actual_values, predicted_values)

# Calculate Youden's J statistic for each threshold
youden_j = tpr - fpr
best_threshold_index = np.argmax(youden_j)
best_threshold = thresholds[best_threshold_index]

# Print the best threshold
print(f"Best Threshold: {best_threshold}")

# Compute the Area Under the Curve (AUC)
roc_auc = auc(fpr, tpr)

# Plot the ROC curve
plt.figure(figsize=(10, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.scatter(fpr[best_threshold_index], tpr[best_threshold_index], marker='o', color='red', label=f'Best Threshold: {best_threshold:.2f}')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Semantic ROC CURVE')
plt.legend(loc='lower right')
plt.show()

threshold_value = 0.002448217983979559 # Set your desired threshold here

# Extract the numeric part of the matrix
numeric_matrix111 = predict11.iloc[:, 1:].values.astype(float)

# Apply thresholding
thresholded_matrix11 = np.where(numeric_matrix111 >= threshold_value, 1, 0)

# Create a new DataFrame from the thresholded matrix with the same index and columns
thresholded_df11 = pd.DataFrame(thresholded_matrix11, index=predict1.index, columns=predict1.columns[1:])

# Display the thresholded DataFrame
print(thresholded_df11)

thresholded_df11.to_excel('thresholded_SEM.xlsx')

import numpy as np
import pandas as pd


threshold_value = 0.002448217983979559  # Set your desired threshold here

# Extract the numeric part of the matrix
numeric_matrix = df.values.astype(float)

# Apply thresholding
thresholded_matrix = np.where(numeric_matrix >= threshold_value, 1, 0)

# Create a new DataFrame from the thresholded matrix with the same index and columns
thresholded_df = pd.DataFrame(thresholded_matrix, index=df.index, columns=df.columns)

# Display the thresholded DataFrame
print(thresholded_df)

# Save the thresholded DataFrame to an Excel file
thresholded_df.to_excel('thresholded_matrix.xlsx')

import numpy as np
import pandas as pd

#file_path1 = '/content/drive/MyDrive/Fun Sim/Functional_Similarity.xlsx'
#Functional_df= pd.read_excel(file_path1).set_index('DOID')
#print(Functional_df)


file_path1 = '/content/drive/MyDrive/Integration/CSM.xlsx'
CSM_df= pd.read_excel(file_path1).set_index('DOID')
print(Functional_df)

file_path8 = '/content/drive/MyDrive/Integration/true positives.xlsx'
actual_df= pd.read_excel(file_path8).set_index('DOID')
#print(actual_df)

file_path3 = '/content/drive/MyDrive/Integration/semfunsim_matrix_final - Copy.xlsx'
Sem_df = pd.read_excel(file_path3).set_index('DOID')
print(Sem_df)

# Perform element-wise multiplication excluding the index column
result = ((0.4 * CSM_df.values) + (0.6 * Sem_df.values))

# Create a new DataFrame with the same index and columns
result_df = pd.DataFrame(result, index=Sem_df.index, columns=Sem_df.columns)

# Normalize the similarity matrix
normalized_similarity_matrix121 = min_max_normalize(result_df)

# Create a DataFrame for the normalized matrix
predict121 = pd.DataFrame(normalized_similarity_matrix121, columns=CSM_df.columns, index=CSM_df.index)
#print(predict121)


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc

# Assuming you have actual_df and predicted_df dataframes
actual_values = actual_df.values.flatten()
predicted_values = predict121.values.flatten()

# Compute the ROC curve
fpr, tpr, thresholds = roc_curve(actual_values, predicted_values)

# Calculate Youden's J statistic for each threshold
youden_j = tpr - fpr
best_threshold_index = np.argmax(youden_j)
best_threshold = thresholds[best_threshold_index]

# Print the best threshold
print(f"Best Threshold: {best_threshold}")

# Compute the Area Under the Curve (AUC)
roc_auc = auc(fpr, tpr)

# Plot the ROC curve
plt.figure(figsize=(10, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.scatter(fpr[best_threshold_index], tpr[best_threshold_index], marker='o', color='red', label=f'Best Threshold: {best_threshold:.2f}')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('(0.7 * Network) * (0.3 * Semantic) ROC CURVE')
plt.legend(loc='lower right')
plt.show()

